{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T18:04:38.975456Z",
     "start_time": "2018-04-12T18:04:38.969451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, datetime\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "SEED = 1337\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T23:29:22.998400Z",
     "start_time": "2018-04-12T23:29:22.479093Z"
    },
    "code_folding": [
     198
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, traceback, os, re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from glob import glob\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 train_path,\n",
    "                 test_path,\n",
    "                 logger_path,\n",
    "                 model_name,\n",
    "                 train_log_interval,\n",
    "                 valid_log_interval,\n",
    "                 load_model_name=None,\n",
    "                 use_script=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        # Data Reader\n",
    "        self.datareader = DataReader(train_path,\n",
    "                                     test_path,\n",
    "                                     **kwargs)\n",
    "        # File Logger\n",
    "        self.filelogger = FileLogger(logger_path,\n",
    "                                     model_name,\n",
    "                                     load_model_name,\n",
    "                                     use_script)\n",
    "\n",
    "        # Check cuda availability\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Variables\n",
    "        self.logger_path = logger_path\n",
    "        self.model_name = model_name\n",
    "        self.train_log_interval = train_log_interval\n",
    "        self.valid_log_interval = valid_log_interval\n",
    "\n",
    "    def save(self,\n",
    "             model_name):\n",
    "        \"\"\"\n",
    "        Save model\n",
    "        \"\"\"\n",
    "        path = self.filelogger.path + '/model_checkpoint/'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.model, path + model_name)\n",
    "\n",
    "    def load(self,\n",
    "             path_name):\n",
    "        \"\"\"\n",
    "        Load model\n",
    "        \"\"\"\n",
    "        print('Loading file from {}'.format(path_name))\n",
    "        self.model = torch.load(path_name)\n",
    "        if self.use_cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "    def train(self,\n",
    "              patience):\n",
    "\n",
    "        self.datareader.calculate_unique_turbines()\n",
    "    \n",
    "        self.datareader.prepare_datareader(self.batch_size, self.validation_split, self.number_steps_train, self.normalizer)\n",
    "\n",
    "        self.filelogger.start()\n",
    "\n",
    "        self.tensorboard = SummaryWriter(self.filelogger.path + '/tensorboard/')\n",
    "        try:\n",
    "            training_step = 0\n",
    "            validation_step = 0\n",
    "\n",
    "            best_validation_loss = 1000\n",
    "            validation_loss = 1000\n",
    "            train_loss = 1000\n",
    "            best_validation_epoch = 0\n",
    "\n",
    "            patience_step = 0\n",
    "\n",
    "            epoch_range = trange(int(self.num_epoch),\n",
    "                                  desc='1st loop',\n",
    "                                  unit=' Epochs')\n",
    "\n",
    "            for epoch in epoch_range:\n",
    "                batch_train_range = trange(int(self.datareader.train_steps),\n",
    "                                            desc='2st loop',\n",
    "                                            unit=' Batch',\n",
    "                                            leave=True)\n",
    "\n",
    "                batch_valid_range = trange(int(self.datareader.validation_steps),\n",
    "                                            desc='2st loop',\n",
    "                                            unit=' Batch',\n",
    "                                            leave=True)\n",
    "\n",
    "                total_train_loss = 0\n",
    "\n",
    "                for batch_train in batch_train_range:\n",
    "                    batch_train_range.set_description(\"Training on %i points --- \" % self.datareader.train_length)\n",
    "\n",
    "                    self.model.train()\n",
    "\n",
    "                    loss, total_loss = self.training_step()\n",
    "\n",
    "                    total_train_loss += total_loss\n",
    "\n",
    "                    batch_train_range.set_postfix(MSE=loss,\n",
    "                                                  Last_batch_MSE=train_loss,\n",
    "                                                  Epoch=epoch)\n",
    "\n",
    "                    self.tensorboard.add_scalar('Training Mean Squared Error loss per batch',\n",
    "                                                loss,\n",
    "                                                training_step)\n",
    "\n",
    "                    self.filelogger.write_train(self.train_log_interval,\n",
    "                                                training_step,\n",
    "                                                epoch,\n",
    "                                                batch_train,\n",
    "                                                loss)\n",
    "\n",
    "                    training_step += 1\n",
    "\n",
    "                train_loss = total_train_loss / (self.datareader.train_length)\n",
    "\n",
    "                self.tensorboard.add_scalar('Training Mean Squared Error loss per epoch',\n",
    "                                            train_loss,\n",
    "                                            epoch)\n",
    "\n",
    "                total_valid_loss = 0\n",
    "\n",
    "                for batch_valid in batch_valid_range:\n",
    "                    batch_valid_range.set_description(\"Validate on %i points --- \" % self.datareader.validation_length)\n",
    "\n",
    "                    batch_valid_range.set_postfix(Last_Batch_MSE=' {0:.9f} MSE'.format(validation_loss),\n",
    "                                                  Best_MSE=best_validation_loss,\n",
    "                                                  Best_Epoch=best_validation_epoch,\n",
    "                                                  Current_Epoch=epoch)\n",
    "\n",
    "                    self.model.eval()\n",
    "\n",
    "                    valid_loss, total_loss = self.evaluation_step()\n",
    "\n",
    "                    total_valid_loss += total_loss\n",
    "\n",
    "                    self.tensorboard.add_scalar('Validation Mean Squared Error loss per batch',\n",
    "                                                valid_loss,\n",
    "                                                validation_step)\n",
    "\n",
    "                    self.filelogger.write_valid(self.valid_log_interval,\n",
    "                                                validation_step,\n",
    "                                                epoch,\n",
    "                                                batch_valid,\n",
    "                                                valid_loss)\n",
    "                    validation_step += 1\n",
    "\n",
    "                validation_loss = total_valid_loss / (self.datareader.validation_length)\n",
    "\n",
    "                self.tensorboard.add_scalar('Validation Mean Squared Error loss per epoch',\n",
    "                                            validation_loss,\n",
    "                                            epoch)\n",
    "\n",
    "                if self.use_scheduler:\n",
    "                    self.scheduler.step(validation_loss)\n",
    "\n",
    "                if validation_loss < best_validation_loss:\n",
    "                    best_validation_loss = validation_loss\n",
    "                    best_validation_epoch = epoch + 1\n",
    "                    patience_step = 0\n",
    "                    self.save('Model_Checkpoint' + str(epoch + 1) + '_valid_loss_' + str(best_validation_loss) + '.pth')\n",
    "                else:\n",
    "                    patience_step += 1\n",
    "                    if patience_step > patience:\n",
    "                        print('Train is donne, 3 epochs in a row without improving validation loss!')\n",
    "                        return best_validation_loss\n",
    "\n",
    "            print('Train is donne after 10 epochs!')\n",
    "            return best_validation_loss\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            if epoch > 0:\n",
    "                print(\"Shutdown requested...saving and exiting\")\n",
    "                self.save('Model_save_before_exiting_epoch_' + str(epoch + 1) + '_batch_' + str(\n",
    "                    batch_train) + '_batch_valid_' + str(batch_valid) + '.pth')\n",
    "            else:\n",
    "                print('Shutdown requested!')\n",
    "\n",
    "        except Exception:\n",
    "            if epoch > 0:\n",
    "                self.save('Model_save_before_exiting_epoch_' + str(epoch + 1) + '_batch_' + str(\n",
    "                    batch_train) + '_batch_valid_' + str(batch_valid) + '.pth')\n",
    "            traceback.print_exc(file=sys.stdout)\n",
    "            sys.exit(0)\n",
    "\n",
    "    def train_cv(self, number_splits, days, patience):\n",
    "\n",
    "        try:\n",
    "            mean_score = []\n",
    "            cv_train_indexes, cv_val_indexes = self.datareader.cross_validation_time_series(number_splits,\n",
    "                                                                                            days,\n",
    "                                                                                            self.test_date)\n",
    "\n",
    "            for model_number in range(number_splits):\n",
    "\n",
    "                self.filelogger.start('Fold_Number{0}'.format(model_number + 1))\n",
    "                self.tensorboard = SummaryWriter(self.filelogger.path + '/tensorboard/')\n",
    "\n",
    "                self.prepare_datareader_cv(cv_train_indexes[model_number],\n",
    "                                           cv_val_indexes[model_number])\n",
    "\n",
    "                training_step = 0\n",
    "                validation_step = 0\n",
    "\n",
    "                best_validation_loss = 1000\n",
    "                validation_loss = 1000\n",
    "                train_loss = 1000\n",
    "                best_validation_epoch = 0\n",
    "\n",
    "                patience_step = 0\n",
    "\n",
    "                epoch_range = trange(int(self.num_epoch),\n",
    "                                     desc='1st loop',\n",
    "                                     unit=' Epochs',\n",
    "                                     leave=True)\n",
    "\n",
    "                for epoch in epoch_range:\n",
    "                    batch_train_range = trange(int(self.datareader.train_steps),\n",
    "                                               desc='2st loop',\n",
    "                                               unit=' Batch',\n",
    "                                               leave=False)\n",
    "\n",
    "                    batch_valid_range = trange(int(self.datareader.validation_steps),\n",
    "                                               desc='2st loop',\n",
    "                                               unit=' Batch',\n",
    "                                               leave=False)\n",
    "\n",
    "                    total_train_loss = 0\n",
    "                    for batch_train in batch_train_range:\n",
    "                        batch_train_range.set_description(\"Training on %i points --- \" % self.datareader.train_length)\n",
    "\n",
    "                        self.model.train()\n",
    "\n",
    "                        loss, total_loss = self.training_step()\n",
    "\n",
    "                        total_train_loss += total_loss\n",
    "\n",
    "                        batch_train_range.set_postfix(MSE=loss,\n",
    "                                                      Last_batch_MSE=train_loss,\n",
    "                                                      Epoch=epoch)\n",
    "\n",
    "                        self.tensorboard.add_scalar('Training Mean Squared Error loss per batch',\n",
    "                                                    loss,\n",
    "                                                    training_step)\n",
    "\n",
    "                        self.filelogger.write_train(self.train_log_interval,\n",
    "                                                    training_step,\n",
    "                                                    epoch,\n",
    "                                                    batch_train,\n",
    "                                                    loss)\n",
    "\n",
    "                        training_step += 1\n",
    "\n",
    "                    train_loss = total_train_loss / (self.datareader.train_length)\n",
    "\n",
    "                    self.tensorboard.add_scalar('Training Mean Squared Error loss per epoch',\n",
    "                                                train_loss,\n",
    "                                                epoch)\n",
    "\n",
    "                    total_valid_loss = 0\n",
    "\n",
    "                    for batch_valid in batch_valid_range:\n",
    "                        batch_valid_range.set_description(\n",
    "                            \"Validate on %i points --- \" % self.datareader.validation_length)\n",
    "\n",
    "                        batch_valid_range.set_postfix(Last_Batch_MSE=' {0:.9f} MSE'.format(validation_loss),\n",
    "                                                      Best_MSE=best_validation_loss,\n",
    "                                                      Best_Epoch=best_validation_epoch,\n",
    "                                                      Current_Epoch=epoch)\n",
    "\n",
    "                        self.model.eval()\n",
    "\n",
    "                        valid_loss, total_loss = self.evaluation_step()\n",
    "\n",
    "                        total_valid_loss += total_loss\n",
    "\n",
    "                        self.tensorboard.add_scalar('Validation Mean Squared Error loss per batch',\n",
    "                                                    valid_loss,\n",
    "                                                    validation_step)\n",
    "\n",
    "                        self.filelogger.write_valid(self.valid_log_interval,\n",
    "                                                    validation_step,\n",
    "                                                    epoch,\n",
    "                                                    batch_valid,\n",
    "                                                    valid_loss)\n",
    "\n",
    "                        validation_step += 1\n",
    "\n",
    "                    validation_loss = total_valid_loss / (self.datareader.validation_length)\n",
    "\n",
    "                    self.tensorboard.add_scalar('Validation Mean Squared Error loss per epoch',\n",
    "                                                validation_loss,\n",
    "                                                epoch)\n",
    "\n",
    "                    if self.use_scheduler:\n",
    "                        self.scheduler.step(validation_loss)\n",
    "\n",
    "                    if validation_loss < best_validation_loss:\n",
    "                        best_validation_loss = validation_loss\n",
    "                        best_validation_epoch = epoch + 1\n",
    "                        patience_step = 0\n",
    "                        self.save(\n",
    "                            'Model_Checkpoint' + str(epoch + 1) + '_valid_loss_' + str(best_validation_loss) + '.pth')\n",
    "                    else:\n",
    "                        patience_step += 1\n",
    "                        if patience_step > patience:\n",
    "                            break\n",
    "\n",
    "                mean_score.append(best_validation_loss)\n",
    "\n",
    "            return np.mean(mean_score)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print('Shutdown requested!')\n",
    "\n",
    "        except Exception:\n",
    "            traceback.print_exc(file=sys.stdout)\n",
    "            sys.exit(0)\n",
    "\n",
    "    def predict(self):\n",
    "\n",
    "        predictions = []\n",
    "        labels = []\n",
    "\n",
    "        for batch_test in range(self.datareader.test_steps):\n",
    "            self.model.eval()\n",
    "\n",
    "            prediction, Y = self.prediction_step()\n",
    "            predictions.append(prediction.cpu().data.numpy())\n",
    "            labels.append(Y)\n",
    "\n",
    "        return np.concatenate(predictions), np.concatenate(labels)\n",
    "\n",
    "    def postprocess(self, predictions, labels):\n",
    "\n",
    "        print(predictions.shape, labels.shape)\n",
    "        predictions = self.datareader.normalizer.inverse_transform(predictions)\n",
    "        labels = self.datareader.normalizer.inverse_transform(labels)\n",
    "\n",
    "        mse = mean_squared_error(predictions, labels)\n",
    "        mae = mean_absolute_error(predictions, labels)\n",
    "\n",
    "        target = self.datareader.data.iloc[self.datareader.test_indexes[:-1]]\n",
    "        results = target.assign(predictions=pd.Series(mean_predictions(predictions), index=target.index).values)\n",
    "\n",
    "        return results, mse, mae\n",
    "\n",
    "    def get_best(self):\n",
    "\n",
    "        files = glob(self.filelogger.path + '/model_checkpoint/*')\n",
    "\n",
    "        best = 100\n",
    "        for file in files:\n",
    "            number = re.findall('[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?\\d+)?', file)\n",
    "            result = float(number[1])\n",
    "            if result < best:\n",
    "                best = result\n",
    "                best_file = file\n",
    "\n",
    "        self.load(best_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T23:29:23.260120Z",
     "start_time": "2018-04-12T23:29:23.000034Z"
    },
    "code_folding": [
     8
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torch.optim as optim\n",
    "\n",
    "SEED = 1337\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 output_size,\n",
    "                 kernel_size=10,\n",
    "                 num_layers=1,\n",
    "                 hidden_size=10,\n",
    "                 cell_type='LSTM'):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder_cell = None\n",
    "        self.cell_type = cell_type\n",
    "        self.output_size = output_size\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        assert self.cell_type in ['LSTM', 'RNN', 'GRU', 'QRNN', 'TCN', 'DRNN'], \\\n",
    "            'Not Implemented, choose on of the following options - ' \\\n",
    "            'LSTM, RNN, GRU'\n",
    "\n",
    "        if self.cell_type == 'LSTM':\n",
    "            self.encoder_cell = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        if self.cell_type == 'GRU':\n",
    "            self.encoder_cell = nn.GRU(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        if self.cell_type == 'RNN':\n",
    "            self.encoder_cell = nn.RNN(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        if self.cell_type == 'QRNN':\n",
    "            self.encoder_cell = QRNN(self.input_size, self.hidden_size, self.num_layers, self.kernel_size)\n",
    "        if self.cell_type == 'DRNN':\n",
    "            self.encoder_cell = DRNN(self.input_size, self.hidden_size, self.num_layers)  # Batch_First always True\n",
    "        if self.cell_type == 'TCN':\n",
    "            self.encoder_cell = TemporalConvNet(self.input_size, self.hidden_size, self.num_layers,\n",
    "                                                self.kernel_size)\n",
    "\n",
    "        self.output_layer = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        outputs, hidden_state = self.encoder_cell(x,\n",
    "                                                  hidden)  # returns output variable - all hidden states for seq_len, hindden state - last hidden state\n",
    "        outputs = self.output_layer(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, x, hidden=None):\n",
    "\n",
    "        prediction, _ = self.encoder_cell(x, hidden)\n",
    "        prediction = self.output_layer(prediction)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "class RNNTrainer(Trainer):\n",
    "    def __init__(self,\n",
    "                 lr,\n",
    "                 number_steps_train,\n",
    "                 hidden_size,\n",
    "                 num_layers,\n",
    "                 cell_type,\n",
    "                 batch_size,\n",
    "                 num_epoch,\n",
    "                 number_features_input=1,\n",
    "                 number_features_output=1,\n",
    "                 kernel_size=None,\n",
    "                 loss_function='MSE',\n",
    "                 optimizer='Adam',\n",
    "                 normalizer='Standardization',\n",
    "                 use_scheduler=False,\n",
    "                 validation_split=0.2,\n",
    "                 **kwargs):\n",
    "\n",
    "        super(RNNTrainer, self).__init__(**kwargs)\n",
    "\n",
    "        torch.manual_seed(SEED)\n",
    "\n",
    "        # Hyper-parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_type = cell_type\n",
    "        self.number_features_input = number_features_input\n",
    "        self.number_features_output = number_features_output\n",
    "        self.number_steps_train = number_steps_train\n",
    "        self.lr = lr\n",
    "        self.kernel_size = kernel_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epoch = num_epoch\n",
    "        self.use_scheduler = use_scheduler\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.normalizer = normalizer\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        self.file_name = self.filelogger.file_name\n",
    "\n",
    "        # Save metadata model\n",
    "        metadata_key = ['number_steps_train',\n",
    "                        'cell_type',\n",
    "                        'hidden_size',\n",
    "                        'kernel_size',\n",
    "                        'num_layers',\n",
    "                        'lr',\n",
    "                        'batch_size',\n",
    "                        'num_epoch']\n",
    "\n",
    "        metadata_value = [self.number_steps_train,\n",
    "                          self.cell_type,\n",
    "                          self.hidden_size,\n",
    "                          self.kernel_size,\n",
    "                          self.num_layers,\n",
    "                          self.lr,\n",
    "                          self.batch_size,\n",
    "                          self.num_epoch]\n",
    "\n",
    "        metadata_dict = {}\n",
    "        for i in range(len(metadata_key)):\n",
    "            metadata_dict[metadata_key[i]] = metadata_value[i]\n",
    "\n",
    "        # check if it's to load model or not\n",
    "        if self.filelogger.load_model is not None:\n",
    "            self.load(self.filelogger.load_model)\n",
    "            print('Load model from {}'.format(\n",
    "                self.logger_path + self.file_name + 'model_checkpoint/' + self.filelogger.load_model))\n",
    "        else:\n",
    "            self.model = RNNModel(self.number_features_input,\n",
    "                                  self.number_features_output,\n",
    "                                  self.kernel_size,\n",
    "                                  self.num_layers,\n",
    "                                  self.hidden_size,\n",
    "                                  self.cell_type)\n",
    "\n",
    "            print(metadata_dict)\n",
    "            self.filelogger.write_metadata(metadata_dict)\n",
    "\n",
    "        # loss function\n",
    "        if loss_function == 'MSE':\n",
    "            self.criterion = nn.MSELoss()\n",
    "\n",
    "        # optimizer\n",
    "        if optimizer == 'Adam':\n",
    "            self.model_optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        elif optimizer == 'SGD':\n",
    "            self.model_optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "        elif optimizer == 'RMSProp':\n",
    "            self.model_optimizer = optim.RMSprop(self.model.parameters(), lr=self.lr)\n",
    "        elif optimizer == 'Adadelta':\n",
    "            self.model_optimizer = optim.Adadelta(self.model.parameters(), lr=self.lr)\n",
    "        elif optimizer == 'Adagrad':\n",
    "            self.model_optimizer = optim.Adagrad(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            self.scheduler = ReduceLROnPlateau(self.model_optimizer, 'min', patience=2, threshold=1e-5)\n",
    "\n",
    "        # check CUDA availability\n",
    "        if self.use_cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "    def training_step(self):\n",
    "\n",
    "        self.model_optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        X, Y = next(self.datareader.train_generator)\n",
    "        length = X.shape[0]\n",
    "        X = Variable(torch.from_numpy(X)).float().cuda()\n",
    "        Y = Variable(torch.from_numpy(Y)).float()\n",
    "\n",
    "        results = self.model(X)\n",
    "\n",
    "        loss = self.criterion(results, Y.unsqueeze(2).cuda())\n",
    "\n",
    "        loss.backward()\n",
    "        self.model_optimizer.step()\n",
    "\n",
    "        return loss.data[0], loss.data[0] * length\n",
    "\n",
    "    def evaluation_step(self):\n",
    "\n",
    "        X, Y = next(self.datareader.validation_generator)\n",
    "        length = X.shape[0]\n",
    "        X = Variable(torch.from_numpy(X), requires_grad=False, volatile=True).float().cuda()\n",
    "        Y = Variable(torch.from_numpy(Y), requires_grad=False, volatile=True).float().cuda()\n",
    "\n",
    "        results = self.model.predict(X)\n",
    "\n",
    "        valid_loss = self.criterion(results, Y.unsqueeze(2).cuda())\n",
    "\n",
    "        return valid_loss.data[0], valid_loss.data[0] * length\n",
    "\n",
    "    def prediction_step(self):\n",
    "\n",
    "        X, Y = next(self.datareader.test_generator)\n",
    "        X = Variable(torch.from_numpy(X), requires_grad=False, volatile=True).float().cuda()\n",
    "\n",
    "        results = self.model.predict(X)\n",
    "\n",
    "        return results, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T23:29:23.450008Z",
     "start_time": "2018-04-12T23:29:23.261871Z"
    },
    "code_folding": [
     5
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, json, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FileLogger(object):\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 file_name,\n",
    "                 model_name,\n",
    "                 script):\n",
    "\n",
    "        if script is not True:\n",
    "            self.path = path + file_name + '/'\n",
    "            self.load_model = None\n",
    "\n",
    "            if not os.path.exists(self.path):\n",
    "                os.makedirs(self.path)\n",
    "\n",
    "            else:\n",
    "                while True:\n",
    "                    overwrite = input('This file already exists. Do you want to overwrite it?'\n",
    "                                      ' Y(yes) N(no) C(continue writing) E(exit)')\n",
    "                    if overwrite == 'Y' or overwrite == 'N' or overwrite == 'C':\n",
    "                        break\n",
    "                    elif overwrite == 'E':\n",
    "                        raise SystemExit\n",
    "                    else:\n",
    "                        print('Please choose one valid option. '\n",
    "                              'Y for yes N for no or C for continue writing')\n",
    "\n",
    "                if overwrite == 'N':\n",
    "                    name = input('Choose a new name for the file:')\n",
    "                    assert os.path.exists(path + name + '/') is False, \\\n",
    "                        'Yeah that one is already in use. Sorry dude! Please choose another name'\n",
    "                    file_name = name\n",
    "                    self.path = path + file_name + '/'\n",
    "                    os.makedirs(self.path)\n",
    "                    print('New directory created at {}'.format(self.path))\n",
    "\n",
    "                if overwrite == 'Y':\n",
    "                    while True:\n",
    "                        overwrite = input('Are you completly fine with this. '\n",
    "                                          'This will remove all previous files and existing models from this directory? Y(yes) N(no)')\n",
    "                        if overwrite == 'Y' or overwrite == 'N':\n",
    "                            break\n",
    "                    if overwrite == 'Y':\n",
    "                        shutil.rmtree(self.path)\n",
    "                        os.makedirs(self.path)\n",
    "\n",
    "                    else:\n",
    "                        assert overwrite is None, \\\n",
    "                            'Well then you should check your files before.' \\\n",
    "                            ' You always can choose another file name to this model'\n",
    "\n",
    "                if overwrite == 'C':\n",
    "                    assert model_name is not None, \\\n",
    "                        'You didnt choose a model to resume train. ' \\\n",
    "                        'Please try again with a valid name or start a new session.'\n",
    "                    assert os.path.exists(path + file_name + '/model_checkpoint/') is True, \\\n",
    "                        'You dont have models to resume. Please restart and start a new session'\n",
    "                    assert os.path.exists(path + file_name + '/model_checkpoint/' + model_name) is True, \\\n",
    "                        'That model name dont exist. Please choose other model to resume'\n",
    "                    self.load_model = path + file_name + '/model_checkpoint/' + model_name\n",
    "        else:\n",
    "            self.path = path + '/' + file_name\n",
    "            self.file_path = self.path\n",
    "            self.load_model = None\n",
    "\n",
    "            if not os.path.exists(self.path):\n",
    "                os.makedirs(self.path)\n",
    "            else:\n",
    "                print('removing')\n",
    "                shutil.rmtree(self.path)\n",
    "                os.makedirs(self.path)\n",
    "\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def write_train(self,\n",
    "                    log_interval,\n",
    "                    step,\n",
    "                    epoch,\n",
    "                    batch,\n",
    "                    loss):\n",
    "\n",
    "        if batch % log_interval == 0:\n",
    "            self.update_file(step,\n",
    "                             epoch,\n",
    "                             batch,\n",
    "                             loss,\n",
    "                             'train_log.txt')\n",
    "\n",
    "    def write_valid(self,\n",
    "                    log_interval,\n",
    "                    step,\n",
    "                    epoch,\n",
    "                    batch,\n",
    "                    loss):\n",
    "\n",
    "        if batch % log_interval == 0:\n",
    "            self.update_file(step,\n",
    "                             epoch,\n",
    "                             batch,\n",
    "                             loss,\n",
    "                             'valid_log.txt')\n",
    "\n",
    "    def write_test(self,\n",
    "                   log_interval,\n",
    "                   step,\n",
    "                   epoch,\n",
    "                   batch,\n",
    "                   loss):\n",
    "\n",
    "        if batch % log_interval == 0:\n",
    "            self.update_file(step,\n",
    "                             epoch,\n",
    "                             batch,\n",
    "                             loss,\n",
    "                             'test_log.txt')\n",
    "\n",
    "    def write_metadata(self,\n",
    "                       metadata):\n",
    "\n",
    "        self.metadataLogger = open(self.path + '/metadata.txt', 'w')\n",
    "        self.metadataLogger.write(json.dumps(metadata))\n",
    "        self.metadataLogger.close()\n",
    "\n",
    "    def open_writers(self):\n",
    "\n",
    "        data = {'Step': [],\n",
    "                'Epoch_Number': [],\n",
    "                'Batch_number': [],\n",
    "                'Loss': []\n",
    "                }\n",
    "\n",
    "        self.trainLogger = open(self.path + '/train_log.txt', 'w')\n",
    "        self.trainLogger.write(json.dumps(data))\n",
    "        self.trainLogger.close()\n",
    "        self.validLogger = open(self.path + '/valid_log.txt', 'w')\n",
    "        self.validLogger.write(json.dumps(data))\n",
    "        self.validLogger.close()\n",
    "        self.testLogger = open(self.path + '/test_log.txt', 'w')\n",
    "        self.testLogger.write(json.dumps(data))\n",
    "        self.testLogger.close()\n",
    "\n",
    "    def update_file(self,\n",
    "                    step,\n",
    "                    epoch,\n",
    "                    batch,\n",
    "                    loss,\n",
    "                    file_name):\n",
    "\n",
    "        data_temp = {\n",
    "            'Step': [int(step)],\n",
    "            'Epoch_Number': [int(epoch)],\n",
    "            'Batch_number': [int(batch)],\n",
    "            'Loss': [float(loss)]\n",
    "        }\n",
    "\n",
    "        with open(self.path + '/' + file_name, 'r') as file:\n",
    "            data = (json.load(file))\n",
    "\n",
    "        for key, value in zip(data.items(), data_temp.items()):\n",
    "            key[1].append(value[1][0])\n",
    "\n",
    "        with open(self.path + '/' + file_name, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "\n",
    "    def read_files(self,\n",
    "                   file_name):\n",
    "\n",
    "        with open(self.path + '/' + file_name, 'r') as file:\n",
    "            data = (json.load(file))\n",
    "\n",
    "        dataframe = []\n",
    "        for column in data.keys():\n",
    "            dataframe.append(pd.DataFrame(data[column], columns=[column]))\n",
    "\n",
    "        return pd.concat(dataframe, axis=1)\n",
    "\n",
    "    def start(self,\n",
    "              name=None):\n",
    "\n",
    "        if name is not None:\n",
    "\n",
    "            self.path = self.file_path + '/' + name\n",
    "\n",
    "            if not os.path.exists(self.path):\n",
    "                os.makedirs(self.path)\n",
    "                self.open_writers()\n",
    "            else:\n",
    "                print('removing')\n",
    "                shutil.rmtree(self.path)\n",
    "                os.makedirs(self.path)\n",
    "                self.open_writers()\n",
    "        else:\n",
    "            self.open_writers()\n",
    "\n",
    "    def write_results(self,\n",
    "                      predictions,\n",
    "                      labels,\n",
    "                      dataframe,\n",
    "                      mse,\n",
    "                      mae):\n",
    "\n",
    "        np.save(self.path + '/predictions.npy', predictions)\n",
    "        np.save(self.path + '/labels.npy', labels)\n",
    "\n",
    "        dataframe.to_csv(self.path + '/results.csv')\n",
    "\n",
    "        with open(self.path + '/final_results.txt', 'w') as file:\n",
    "            file.write('Mean Squared Error - {}\\n'.format(mse))\n",
    "            file.write('Mean Absolute Error - {}'.format(mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T23:29:24.632533Z",
     "start_time": "2018-04-12T23:29:23.677028Z"
    },
    "code_folding": [
     208,
     279,
     292,
     325,
     328,
     331,
     337
    ]
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "SEED = 1337\n",
    "\n",
    "sensor_columns = [\"sensor {}\".format(s) for s in range(1,22)]\n",
    "\n",
    "info_columns = ['dataset_id', 'unit_id','cycle','setting 1', 'setting 2', 'setting 3']\n",
    "\n",
    "label_columns = ['dataset_id', 'unit_id', 'rul']\n",
    "\n",
    "settings = ['setting 1', 'setting 2', 'setting 3']\n",
    "type_1 = ['FD001', 'FD003']\n",
    "type_2 = ['FD002', 'FD004']\n",
    "\n",
    "class DataReader(object):\n",
    "    def __init__(self,\n",
    "                 raw_data_path_train,\n",
    "                 raw_data_path_test,\n",
    "                 **kwargs):\n",
    "\n",
    "        assert os.path.isfile(raw_data_path_train) is True, \\\n",
    "            'This file do not exist. Please select an existing file'\n",
    "\n",
    "        assert os.path.isfile(raw_data_path_test) is True, \\\n",
    "            'This file do not exist. Please select an existing file'\n",
    "\n",
    "        assert raw_data_path_train.lower().endswith(('.csv', '.parquet', '.hdf5', '.pickle', '.pkl')) is True, \\\n",
    "            'This class can\\'t handle this extension. Please specify a .csv, .parquet, .hdf5, .pickle extension'\n",
    "\n",
    "        assert raw_data_path_test.lower().endswith(('.csv', '.parquet', '.hdf5', '.pickle', 'pkl')) is True, \\\n",
    "            'This class can\\'t handle this extension. Please specify a .csv, .parquet, .hdf5, .pickle extension'\n",
    "\n",
    "        self.raw_data_path_train = raw_data_path_train\n",
    "        self.raw_data_path_test = raw_data_path_test\n",
    "        self.loader_engine(**kwargs)\n",
    "        self.train = self.loader_train()\n",
    "        self.test = self.loader_test()\n",
    "\n",
    "    def loader_engine(self, **kwargs):\n",
    "        if self.raw_data_path_train.lower().endswith(('.csv')):\n",
    "            self.loader_train = lambda: pd.read_csv(self.raw_data_path_train, **kwargs)\n",
    "            self.loader_test = lambda: pd.read_csv(self.raw_data_path_test, **kwargs)\n",
    "        elif self.raw_data_path_train.lower().endswith(('.parquet')):\n",
    "            self.loader_train = lambda: pd.read_parquet(self.raw_data_path_train, **kwargs)\n",
    "            self.loader_test = lambda: pd.read_parquet(self.raw_data_path_test, **kwargs)\n",
    "        elif self.raw_data_path_train.lower().endswith(('.hdf5')):\n",
    "            self.loader_train = lambda: pd.read_hdf(self.raw_data_path_train, **kwargs)\n",
    "            self.loader_test = lambda: pd.read_hdf(self.raw_data_path_test, **kwargs)\n",
    "        elif self.raw_data_path_train.lower().endswith(('.pkl', 'pickle')):\n",
    "            self.loader_train = lambda: pd.read_pickle(self.raw_data_path_train, **kwargs)\n",
    "            self.loader_test = lambda: pd.read_pickle(self.raw_data_path_test, **kwargs)\n",
    "\n",
    "    def calculate_unique_turbines(self):\n",
    "\n",
    "        self.train_turbines = np.arange(len(self.train.index.to_series().unique()))\n",
    "        self.test_turbines = np.arange(len(self.test.index.to_series().unique()))\n",
    "\n",
    "    def cluestering(self, train, validation, test=None, min_cluster_size=100):\n",
    "\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True).fit(\n",
    "            train[['setting 1', 'setting 2', 'setting 3']])\n",
    "\n",
    "        train_labels, strengths = hdbscan.approximate_predict(clusterer, train[['setting 1', 'setting 2', 'setting 3']])\n",
    "        validation_labels, strengths = hdbscan.approximate_predict(clusterer,\n",
    "                                                                   validation[['setting 1', 'setting 2', 'setting 3']])\n",
    "\n",
    "        train['HDBScan'] = train_labels\n",
    "        validation['HDBScan'] = validation_labels\n",
    "\n",
    "        if test is not None:\n",
    "            test_labels, strengths = hdbscan.approximate_predict(clusterer,\n",
    "                                                                 test[['setting 1', 'setting 2', 'setting 3']])\n",
    "            test['HDBScan'] = test_labels\n",
    "\n",
    "            return train, validation, test\n",
    "        else:\n",
    "            return train, validation\n",
    "\n",
    "    def normalize_by_type(self, train, validation, normalization, test=None):\n",
    "\n",
    "        df_train_type1 = train.loc[type_1]\n",
    "        df_train_type2 = train.loc[type_2]\n",
    "\n",
    "        df_validation_type1 = validation.loc[type_1]\n",
    "        df_validation_type2 = validation.loc[type_2]\n",
    "\n",
    "        df_train_type1_normalize = df_train_type1.copy()\n",
    "        df_validation_type1_normalize = df_validation_type1.copy()\n",
    "        \n",
    "        if normalization == 'Standardization':\n",
    "            scaler_type1 = StandardScaler().fit(df_train_type1[sensor_columns])\n",
    "        elif normalization == 'MinMaxScaler':\n",
    "            scaler_type1 = MinMaxScaler().fit(df_train_type1[sensor_columns])\n",
    "        \n",
    "        df_train_type1_normalize[sensor_columns] = scaler_type1.transform(df_train_type1[sensor_columns])\n",
    "        df_validation_type1_normalize[sensor_columns] = scaler_type1.transform(df_validation_type1[sensor_columns])\n",
    "\n",
    "        df_train_type1 = df_train_type1_normalize.copy()\n",
    "        df_validation_type1 = df_validation_type1_normalize.copy()\n",
    "\n",
    "        del (df_train_type1_normalize, df_validation_type1_normalize)\n",
    "\n",
    "        df_train_type2_normalize = df_train_type2.copy()\n",
    "        df_validation_type2_normalize = df_validation_type2.copy()\n",
    "\n",
    "        gb = df_train_type2.groupby('HDBScan')[sensor_columns]\n",
    "\n",
    "        d = {}\n",
    "\n",
    "        for x in gb.groups:\n",
    "            if normalization == 'Standardization':\n",
    "                d[\"scaler_type2_{0}\".format(x)] = StandardScaler().fit(gb.get_group(x))\n",
    "            elif normalization == 'MinMaxScaler':\n",
    "                d[\"scaler_type2_{0}\".format(x)] = StandardScaler().fit(gb.get_group(x))\n",
    "\n",
    "            df_train_type2_normalize.loc[df_train_type2_normalize['HDBScan'] == x, sensor_columns] = d[\n",
    "                \"scaler_type2_{0}\".format(x)].transform(\n",
    "                df_train_type2.loc[df_train_type2['HDBScan'] == x, sensor_columns])\n",
    "            df_validation_type2_normalize.loc[df_validation_type2_normalize['HDBScan'] == x, sensor_columns] = d[\n",
    "                \"scaler_type2_{0}\".format(x)].transform(\n",
    "                df_validation_type2.loc[df_validation_type2['HDBScan'] == x, sensor_columns])\n",
    "\n",
    "        df_train_type2 = df_train_type2_normalize.copy()\n",
    "        df_validation_type2 = df_validation_type2_normalize.copy()\n",
    "\n",
    "        del (df_train_type2_normalize, df_validation_type2_normalize)\n",
    "\n",
    "        df_train_all = pd.concat([df_train_type1, df_train_type2])\n",
    "        df_validation_all = pd.concat([df_validation_type1, df_validation_type2])\n",
    "\n",
    "        if test is not None:\n",
    "            df_test_type1 = test.loc[type_1]\n",
    "            df_test_type2 = test.loc[type_2]\n",
    "\n",
    "            df_test_type1_normalize = df_test_type1.copy()\n",
    "            df_test_type1_normalize[sensor_columns] = scaler_type1.transform(df_test_type1[sensor_columns])\n",
    "            df_test_type1 = df_test_type1_normalize.copy()\n",
    "\n",
    "            del(df_test_type1_normalize)\n",
    "\n",
    "            df_test_type2_normalize = df_test_type2.copy()\n",
    "\n",
    "            for x in gb.groups:\n",
    "                df_test_type2_normalize.loc[df_test_type2_normalize['HDBScan'] == x, sensor_columns] = d[\n",
    "                    \"scaler_type2_{0}\".format(x)].transform(\n",
    "                    df_test_type2.loc[df_test_type2['HDBScan'] == x, sensor_columns])\n",
    "\n",
    "            df_test_type2 = df_test_type2_normalize.copy()\n",
    "\n",
    "            del(df_test_type2_normalize)\n",
    "\n",
    "            df_test_all = pd.concat([df_test_type1, df_test_type2])\n",
    "\n",
    "            return df_train_all, df_validation_all, df_test_all\n",
    "        else:\n",
    "            return df_train_all, df_validation_all\n",
    "        \n",
    "        \n",
    "    def binarize(self, train, validation, test=None):\n",
    "\n",
    "        setting_operational = [\"setting_op {}\".format(s) for s in range(1, 7)]\n",
    "        dataset_id_columns = [\"dataset_id {}\".format(s) for s in range(1, 5)]\n",
    "\n",
    "        preprocess_HDBscan = LabelBinarizer()\n",
    "        preprocess_ID = LabelBinarizer()\n",
    "\n",
    "        preprocess_HDBscan.fit(train['HDBScan'])\n",
    "        preprocess_ID.fit(train.reset_index()['dataset_id'])\n",
    "\n",
    "        dataframe_HDBscan = pd.DataFrame(preprocess_HDBscan.transform(train['HDBScan']),\n",
    "                                         columns=setting_operational)\n",
    "        dataframe_dataset_id = pd.DataFrame(preprocess_ID.transform(train.reset_index()['dataset_id']),\n",
    "                                            columns=dataset_id_columns)\n",
    "\n",
    "        dataframe_HDBscan_validation = pd.DataFrame(preprocess_HDBscan.transform(validation['HDBScan']),\n",
    "                                                    columns=setting_operational)\n",
    "        dataframe_dataset_id_validation = pd.DataFrame(\n",
    "            preprocess_ID.transform(validation.reset_index()['dataset_id']), columns=dataset_id_columns)\n",
    "\n",
    "        train = train.reset_index().join(dataframe_HDBscan)\n",
    "        train = train.join(dataframe_dataset_id)\n",
    "\n",
    "        validation = validation.reset_index().join(dataframe_HDBscan_validation)\n",
    "        validation = validation.join(dataframe_dataset_id_validation)\n",
    "\n",
    "        if test is not None:\n",
    "            dataframe_HDBscan_test = pd.DataFrame(preprocess_HDBscan.transform(test['HDBScan']),\n",
    "                                                  columns=setting_operational)\n",
    "            dataframe_dataset_id_test = pd.DataFrame(preprocess_ID.transform(test.reset_index()['dataset_id']),\n",
    "                                                     columns=dataset_id_columns)\n",
    "\n",
    "            test = test.reset_index().join(dataframe_HDBscan_test)\n",
    "            test = test.join(dataframe_dataset_id_test)\n",
    "\n",
    "            return train, validation, test\n",
    "        else:\n",
    "            return train, validation\n",
    "\n",
    "    def transform_data(self, df, length_sequence):\n",
    "\n",
    "        array_data = []\n",
    "        array_data_label = []\n",
    "\n",
    "        for index_train in (df.index.to_series().unique()):\n",
    "            temp_df_train = df.loc[index_train]\n",
    "\n",
    "            for i in range(1, len(temp_df_train)):\n",
    "                train_x = np.ones((length_sequence, (len(temp_df_train.columns) - 1))) * -1000\n",
    "                train_y = np.ones((length_sequence, 1)) * -1000\n",
    "\n",
    "                if i - length_sequence < 0:\n",
    "                    x = 0\n",
    "                else:\n",
    "                    x = i - length_sequence\n",
    "\n",
    "                data = temp_df_train.iloc[x:i]\n",
    "\n",
    "                label = data['RUL'].copy().values\n",
    "                data = data.drop(['RUL'], axis=1).values\n",
    "                train_x[-len(data):, :] = data\n",
    "\n",
    "                train_y[-len(data):, 0] = label\n",
    "                array_data.append(train_x)\n",
    "                array_data_label.append(train_y)\n",
    "\n",
    "        return np.array(array_data), np.array(array_data_label)\n",
    "\n",
    "    def prepare_datareader(self, batch_size, validation_split, number_steps_train, normalization):\n",
    "\n",
    "        train_turbines, validation_turbines = train_test_split(self.train_turbines, test_size=validation_split)\n",
    "\n",
    "        idx_train = self.train.index.to_series().unique()[train_turbines]\n",
    "        idx_validation = self.train.index.to_series().unique()[validation_turbines]\n",
    "        idx_test = self.test.index.to_series().unique()[self.test_turbines]\n",
    "\n",
    "        train = self.train.loc[idx_train]\n",
    "        validation = self.train.loc[idx_validation]\n",
    "        test = self.test.loc[idx_test]\n",
    "\n",
    "        train, validation, test = self.normalize_by_type(train, validation, normalization, test)\n",
    "\n",
    "        train, validation, test = self.binarize(train, validation, test)\n",
    "\n",
    "        train = train.set_index(['dataset_id', 'unit_id']).drop(\n",
    "            ['HDBScan', 'cycle', 'setting 1', 'setting 2', 'setting 3',\n",
    "             'sensor 1', 'sensor 5', 'sensor 10', 'sensor 16', 'sensor 18', 'sensor 19'], axis=1)\n",
    "        validation = validation.set_index(['dataset_id', 'unit_id']).drop(\n",
    "            ['HDBScan', 'cycle', 'setting 1', 'setting 2', 'setting 3',\n",
    "             'sensor 1', 'sensor 5', 'sensor 10', 'sensor 16', 'sensor 18', 'sensor 19'], axis=1)\n",
    "        test = test.set_index(['dataset_id', 'unit_id']).drop(\n",
    "            ['HDBScan', 'cycle', 'setting 1', 'setting 2', 'setting 3',\n",
    "             'sensor 1', 'sensor 5', 'sensor 10', 'sensor 16', 'sensor 18', 'sensor 19'], axis=1)\n",
    "\n",
    "        self.train_data, self.train_label_data = self.transform_data(train, number_steps_train)\n",
    "        self.validation_data, self.validation_label_data  = self.transform_data(validation, number_steps_train)\n",
    "        self.test_data, self.test_label_data = self.transform_data(test, number_steps_train)\n",
    "\n",
    "        self.train_length = len(self.train_data)\n",
    "        self.validation_length = len(self.validation_data)\n",
    "        self.test_length = len(self.test_data)\n",
    "\n",
    "        self.train_steps = round(len(self.train_data) / batch_size + 0.5)\n",
    "        self.validation_steps = round(len(self.validation_data) / batch_size + 0.5)\n",
    "        self.test_steps = round(len(self.test_data) / batch_size + 0.5)\n",
    "\n",
    "        self.train_generator = self.generator_train(batch_size)\n",
    "        self.validation_generator = self.generator_validation(batch_size)\n",
    "        self.test_generator = self.generator_test(batch_size)\n",
    "\n",
    "    def calculate_unique_turbines_cv(self, splits=5):\n",
    "\n",
    "        cv = []\n",
    "        cv_val = []\n",
    "\n",
    "        split = KFold(n_splits=splits, shuffle=True)\n",
    "\n",
    "        for train_index, validation_index in split.split(np.arange(len(self.train.index.to_series().unique()))):\n",
    "            cv.append(train_index)\n",
    "            cv_val.append(validation_index)\n",
    "\n",
    "        return cv, cv_val\n",
    "\n",
    "    def prepare_datareader_cv(self, splits, batch_size, number_steps_train):\n",
    "\n",
    "        cv_indexes, cv_val_indexes = self.calculate_unique_turbines_cv(splits)\n",
    "\n",
    "        idx_train = self.train.index.to_series().unique()[cv_indexes]\n",
    "        idx_val = self.train.index.to_series().unique()[cv_val_indexes]\n",
    "\n",
    "        train = self.train.loc[idx_train]\n",
    "        validation = self.train.loc[idx_val]\n",
    "\n",
    "        train, validation = self.normalize_by_type(train, validation)\n",
    "\n",
    "        train, validation = self.binarize(train, validation)\n",
    "\n",
    "        train = train.set_index(['dataset_id', 'unit_id']).drop(\n",
    "            ['HDBScan', 'cycle', 'setting 1', 'setting 2', 'setting 3',\n",
    "             'sensor 1', 'sensor 5', 'sensor 10', 'sensor 16', 'sensor 18', 'sensor 19'], axis=1)\n",
    "        validation = validation.set_index(['dataset_id', 'unit_id']).drop(\n",
    "            ['HDBScan', 'cycle', 'setting 1', 'setting 2', 'setting 3',\n",
    "             'sensor 1', 'sensor 5', 'sensor 10', 'sensor 16', 'sensor 18', 'sensor 19'], axis=1)\n",
    "\n",
    "        self.train_data, self.train_label_data = self.transform_data(train, number_steps_train)\n",
    "        self.validation_data, self.validation_label_data = self.transform_data(validation, number_steps_train)\n",
    "\n",
    "        self.train_length = len(self.train_data)\n",
    "        self.validation_length = len(self.validation_data)\n",
    "\n",
    "        self.train_steps = round(len(self.train_data) / batch_size + 0.5)\n",
    "        self.validation_steps = round(len(self.validation_data) / batch_size + 0.5)\n",
    "\n",
    "        self.train_generator = self.generator_train(batch_size)\n",
    "        self.validation_generator = self.generator_validation(batch_size)\n",
    "\n",
    "    def generator_train(self, batch_size):\n",
    "        while True:\n",
    "            self.train_data, self.train_label_data = shuffle(self.train_data, self.train_label_data, random_state=1337)\n",
    "            for ndx in range(0, self.train_length, batch_size):\n",
    "                yield self.train_data[ndx:min(ndx + batch_size, self.train_length)], self.train_label_data[ndx:min(ndx + batch_size, self.train_length)]\n",
    "\n",
    "    def generator_validation(self, batch_size):\n",
    "        while True:\n",
    "            for ndx in range(0, self.validation_length, batch_size):\n",
    "                yield self.validation_data[ndx:min(ndx + batch_size, self.validation_length)], self.validation_label_data[\n",
    "                                                                      ndx:min(ndx + batch_size, self.validation_length)]\n",
    "\n",
    "    def generator_test(self, batch_size):\n",
    "        while True:\n",
    "            for ndx in range(0, self.test_length, batch_size):\n",
    "                yield self.test_data[ndx:min(ndx + batch_size, self.test_length)], self.test_label_data[ndx:min(ndx + batch_size, self.test_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T23:29:24.749054Z",
     "start_time": "2018-04-12T23:29:24.634742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing\n",
      "{'number_steps_train': 100, 'cell_type': 'LSTM', 'hidden_size': 256, 'kernel_size': 20, 'num_layers': 3, 'lr': 0.05, 'batch_size': 256, 'num_epoch': 10}\n"
     ]
    }
   ],
   "source": [
    "model = RNNTrainer(train_path = '/datadrive/Turbofan_Engine/df_train_cluster.pkl',\n",
    "                   test_path = '/datadrive/Turbofan_Engine/df_test_cluster.pkl',\n",
    "                   logger_path = '/home/rneves/temp/temp_logger/',\n",
    "                   model_name = 'Turbofan_Test6',\n",
    "                   train_log_interval = 100,\n",
    "                   valid_log_interval = 100,\n",
    "                   validation_split = 0.2,\n",
    "                   use_script=True,\n",
    "                   lr = 0.05,\n",
    "                   number_steps_train = 100,\n",
    "                   hidden_size = 256,\n",
    "                   num_layers = 3,\n",
    "                   cell_type = 'LSTM',\n",
    "                   kernel_size=20,\n",
    "                   batch_size = 256,\n",
    "                   num_epoch = 10,\n",
    "                   number_features_input = 25,\n",
    "                   number_features_output = 1,\n",
    "                   loss_function = 'MSE',\n",
    "                   optimizer = 'Adam',\n",
    "                   normalizer = 'MinMaxScaler',\n",
    "                   use_scheduler = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T23:27:04.061561Z",
     "start_time": "2018-04-12T23:26:57.522743Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-be4991b2fa46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-191-dcc75a905aac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, patience)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatareader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_unique_turbines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatareader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_datareader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_steps_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilelogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-221-8ddff2e1d013>\u001b[0m in \u001b[0;36mprepare_datareader\u001b[0;34m(self, batch_size, validation_split, number_steps_train, normalization)\u001b[0m\n\u001b[1;32m    258\u001b[0m              'sensor 1', 'sensor 5', 'sensor 10', 'sensor 16', 'sensor 18', 'sensor 19'], axis=1)\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_label_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_steps_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_label_data\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_steps_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_label_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_steps_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-221-8ddff2e1d013>\u001b[0m in \u001b[0;36mtransform_data\u001b[0;34m(self, df, length_sequence)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_df_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RUL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RUL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   4062\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4063\u001b[0m         \"\"\"\n\u001b[0;32m-> 4064\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4065\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep, mgr)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m                 \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m                 \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(ax)\u001b[0m\n\u001b[1;32m   3652\u001b[0m                 \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3654\u001b[0;31m                 \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3655\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mview\u001b[0;34m(self, cls)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;34m\"\"\" this is defined as a copy with the same identity \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, names, dtype, levels, labels, deep, _set_identity, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m         return MultiIndex(levels=levels, labels=labels, names=names,\n\u001b[1;32m    426\u001b[0m                           \u001b[0msortorder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                           _set_identity=_set_identity)\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, levels, labels, sortorder, names, copy, verify_integrity, _set_identity, name, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m# handles name validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msortorder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_set_names\u001b[0;34m(self, names, level, validate)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# set the name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     names = property(fset=_set_names, fget=_get_names,\n",
      "\u001b[0;32m/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mrename\u001b[0;34m(self, name, inplace)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mnew\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mof\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0metc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \"\"\"\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rneves/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mset_names\u001b[0;34m(self, names, level, inplace)\u001b[0m\n\u001b[1;32m   1158\u001b[0m                    names=[u'baz', u'bar'])\n\u001b[1;32m   1159\u001b[0m         \"\"\"\n\u001b[0;32m-> 1160\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Level must be None for non-MultiIndex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T23:08:59.234440Z",
     "start_time": "2018-04-12T23:08:33.391626Z"
    }
   },
   "outputs": [],
   "source": [
    "a, b = model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T23:09:15.655398Z",
     "start_time": "2018-04-12T23:09:15.615266Z"
    }
   },
   "outputs": [],
   "source": [
    "def module(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "c = pd.DataFrame(a[:, 99, 0])[0].apply(lambda x: module(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T23:13:00.277117Z",
     "start_time": "2018-04-12T23:13:00.079639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.839484</td>\n",
       "      <td>-0.623655</td>\n",
       "      <td>-0.902766</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.127820</td>\n",
       "      <td>-0.380492</td>\n",
       "      <td>-0.675883</td>\n",
       "      <td>-0.981141</td>\n",
       "      <td>-0.200647</td>\n",
       "      <td>-0.460733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.660699</td>\n",
       "      <td>-0.143257</td>\n",
       "      <td>-1.192616</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>0.229215</td>\n",
       "      <td>-0.625024</td>\n",
       "      <td>-0.473588</td>\n",
       "      <td>0.052924</td>\n",
       "      <td>-0.024352</td>\n",
       "      <td>-0.216213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.229297</td>\n",
       "      <td>-0.372814</td>\n",
       "      <td>-0.577594</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.048896</td>\n",
       "      <td>-0.298982</td>\n",
       "      <td>-0.352687</td>\n",
       "      <td>0.052924</td>\n",
       "      <td>-0.100479</td>\n",
       "      <td>-0.460733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.267467</td>\n",
       "      <td>-0.801524</td>\n",
       "      <td>-0.049840</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.063929</td>\n",
       "      <td>-0.462003</td>\n",
       "      <td>-0.908165</td>\n",
       "      <td>-0.705390</td>\n",
       "      <td>-0.336875</td>\n",
       "      <td>-0.297719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.133870</td>\n",
       "      <td>-0.334808</td>\n",
       "      <td>-0.517339</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.030105</td>\n",
       "      <td>-0.625024</td>\n",
       "      <td>-0.943388</td>\n",
       "      <td>-0.601984</td>\n",
       "      <td>-0.028359</td>\n",
       "      <td>-0.460733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.897285</td>\n",
       "      <td>-1.561647</td>\n",
       "      <td>-1.222743</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.007555</td>\n",
       "      <td>-0.706535</td>\n",
       "      <td>-0.638280</td>\n",
       "      <td>-0.774328</td>\n",
       "      <td>-0.120513</td>\n",
       "      <td>-0.053199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.897285</td>\n",
       "      <td>-0.920103</td>\n",
       "      <td>-0.213984</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.131578</td>\n",
       "      <td>-0.298982</td>\n",
       "      <td>-0.617812</td>\n",
       "      <td>-0.601984</td>\n",
       "      <td>-0.084453</td>\n",
       "      <td>-0.216213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.076614</td>\n",
       "      <td>-1.292564</td>\n",
       "      <td>-0.624344</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.244325</td>\n",
       "      <td>-0.298982</td>\n",
       "      <td>-0.547842</td>\n",
       "      <td>-0.946672</td>\n",
       "      <td>-0.052399</td>\n",
       "      <td>-0.216213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.336248</td>\n",
       "      <td>0.592542</td>\n",
       "      <td>0.558948</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>0.093918</td>\n",
       "      <td>-0.217471</td>\n",
       "      <td>-0.752993</td>\n",
       "      <td>-0.395171</td>\n",
       "      <td>-0.076439</td>\n",
       "      <td>-0.297719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.973626</td>\n",
       "      <td>-0.629736</td>\n",
       "      <td>-0.858094</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>0.014994</td>\n",
       "      <td>-0.380492</td>\n",
       "      <td>-0.591157</td>\n",
       "      <td>-1.187954</td>\n",
       "      <td>-0.088459</td>\n",
       "      <td>-0.216213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.030882</td>\n",
       "      <td>-1.271280</td>\n",
       "      <td>-0.318911</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>0.169083</td>\n",
       "      <td>-0.380492</td>\n",
       "      <td>-0.604485</td>\n",
       "      <td>-0.877735</td>\n",
       "      <td>-0.108493</td>\n",
       "      <td>-0.216213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.076614</td>\n",
       "      <td>-0.298322</td>\n",
       "      <td>-0.943283</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>0.041302</td>\n",
       "      <td>-0.543514</td>\n",
       "      <td>-0.683023</td>\n",
       "      <td>-0.739859</td>\n",
       "      <td>-0.084453</td>\n",
       "      <td>-0.216213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.221736</td>\n",
       "      <td>-0.045961</td>\n",
       "      <td>-0.307484</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.075204</td>\n",
       "      <td>-0.543514</td>\n",
       "      <td>-0.890078</td>\n",
       "      <td>-0.464108</td>\n",
       "      <td>0.059789</td>\n",
       "      <td>-0.460733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.668260</td>\n",
       "      <td>-0.947468</td>\n",
       "      <td>-0.417606</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>0.157808</td>\n",
       "      <td>-0.462003</td>\n",
       "      <td>-0.907689</td>\n",
       "      <td>-0.843266</td>\n",
       "      <td>-0.108493</td>\n",
       "      <td>-0.216213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.152955</td>\n",
       "      <td>-0.696627</td>\n",
       "      <td>-0.842511</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.033863</td>\n",
       "      <td>-0.706535</td>\n",
       "      <td>-0.560694</td>\n",
       "      <td>-0.464108</td>\n",
       "      <td>-0.336875</td>\n",
       "      <td>-0.705253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.496492</td>\n",
       "      <td>-0.742234</td>\n",
       "      <td>0.089371</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.157886</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>-1.067621</td>\n",
       "      <td>-0.326233</td>\n",
       "      <td>-0.024352</td>\n",
       "      <td>0.109814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.744602</td>\n",
       "      <td>-1.017399</td>\n",
       "      <td>-0.288784</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>0.067610</td>\n",
       "      <td>-0.217471</td>\n",
       "      <td>-0.881510</td>\n",
       "      <td>-0.739859</td>\n",
       "      <td>-0.052399</td>\n",
       "      <td>-0.542239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.018813</td>\n",
       "      <td>-0.435145</td>\n",
       "      <td>-0.333456</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.278150</td>\n",
       "      <td>-0.380492</td>\n",
       "      <td>-0.773461</td>\n",
       "      <td>-0.153889</td>\n",
       "      <td>-0.032365</td>\n",
       "      <td>-0.216213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.286553</td>\n",
       "      <td>-0.579568</td>\n",
       "      <td>-0.477861</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.139094</td>\n",
       "      <td>-0.625024</td>\n",
       "      <td>-0.831531</td>\n",
       "      <td>-0.808797</td>\n",
       "      <td>-0.064419</td>\n",
       "      <td>-0.623746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.056984</td>\n",
       "      <td>-0.245114</td>\n",
       "      <td>-0.644083</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>0.026269</td>\n",
       "      <td>-0.298982</td>\n",
       "      <td>-1.106652</td>\n",
       "      <td>-0.084951</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>-0.297719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.228752</td>\n",
       "      <td>-0.926184</td>\n",
       "      <td>-0.799916</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>0.067610</td>\n",
       "      <td>-0.298982</td>\n",
       "      <td>-0.506431</td>\n",
       "      <td>-0.429640</td>\n",
       "      <td>-0.068426</td>\n",
       "      <td>0.191321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.248382</td>\n",
       "      <td>-1.005237</td>\n",
       "      <td>-0.295017</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.304458</td>\n",
       "      <td>-0.706535</td>\n",
       "      <td>-0.852951</td>\n",
       "      <td>-0.774328</td>\n",
       "      <td>-0.324855</td>\n",
       "      <td>-0.379226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.878199</td>\n",
       "      <td>-0.286160</td>\n",
       "      <td>-1.226899</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>0.311897</td>\n",
       "      <td>-0.380492</td>\n",
       "      <td>-0.585921</td>\n",
       "      <td>-1.015610</td>\n",
       "      <td>-0.088459</td>\n",
       "      <td>-0.216213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.496492</td>\n",
       "      <td>0.744567</td>\n",
       "      <td>-0.701222</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.364590</td>\n",
       "      <td>-0.135960</td>\n",
       "      <td>-1.001935</td>\n",
       "      <td>-0.670922</td>\n",
       "      <td>-0.064419</td>\n",
       "      <td>0.272827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.630089</td>\n",
       "      <td>-1.058446</td>\n",
       "      <td>-0.692911</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.180435</td>\n",
       "      <td>0.190082</td>\n",
       "      <td>-0.979087</td>\n",
       "      <td>-0.395171</td>\n",
       "      <td>0.015715</td>\n",
       "      <td>-0.053199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.191126</td>\n",
       "      <td>-0.929225</td>\n",
       "      <td>0.121577</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>-0.054450</td>\n",
       "      <td>-0.520711</td>\n",
       "      <td>-0.533046</td>\n",
       "      <td>-0.108493</td>\n",
       "      <td>-0.134706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.954541</td>\n",
       "      <td>-0.416902</td>\n",
       "      <td>-0.684600</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>0.041302</td>\n",
       "      <td>0.027061</td>\n",
       "      <td>-0.869610</td>\n",
       "      <td>-0.498577</td>\n",
       "      <td>-0.160580</td>\n",
       "      <td>-0.542239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.240821</td>\n",
       "      <td>0.738486</td>\n",
       "      <td>-0.582789</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.255600</td>\n",
       "      <td>-0.135960</td>\n",
       "      <td>-0.419325</td>\n",
       "      <td>-1.498174</td>\n",
       "      <td>-0.152566</td>\n",
       "      <td>-0.134706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.202651</td>\n",
       "      <td>-0.340889</td>\n",
       "      <td>-0.913155</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.033863</td>\n",
       "      <td>-0.054450</td>\n",
       "      <td>-0.869134</td>\n",
       "      <td>-0.222826</td>\n",
       "      <td>0.067802</td>\n",
       "      <td>-0.134706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.400520</td>\n",
       "      <td>-0.558285</td>\n",
       "      <td>-0.616033</td>\n",
       "      <td>0.470001</td>\n",
       "      <td>-0.052654</td>\n",
       "      <td>0.027061</td>\n",
       "      <td>-0.805352</td>\n",
       "      <td>-0.291764</td>\n",
       "      <td>-0.176607</td>\n",
       "      <td>0.109814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104160</th>\n",
       "      <td>1.035780</td>\n",
       "      <td>0.948075</td>\n",
       "      <td>0.923657</td>\n",
       "      <td>0.695748</td>\n",
       "      <td>-1.104596</td>\n",
       "      <td>0.741602</td>\n",
       "      <td>0.785360</td>\n",
       "      <td>1.116721</td>\n",
       "      <td>-0.351261</td>\n",
       "      <td>0.627223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104161</th>\n",
       "      <td>0.390129</td>\n",
       "      <td>1.319271</td>\n",
       "      <td>1.270024</td>\n",
       "      <td>0.651815</td>\n",
       "      <td>-0.430087</td>\n",
       "      <td>0.577476</td>\n",
       "      <td>1.108957</td>\n",
       "      <td>0.920130</td>\n",
       "      <td>-0.216567</td>\n",
       "      <td>0.778516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104162</th>\n",
       "      <td>0.888398</td>\n",
       "      <td>1.722405</td>\n",
       "      <td>1.172488</td>\n",
       "      <td>-0.183013</td>\n",
       "      <td>-0.792743</td>\n",
       "      <td>0.787074</td>\n",
       "      <td>0.973278</td>\n",
       "      <td>1.253729</td>\n",
       "      <td>-0.406216</td>\n",
       "      <td>0.720315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104163</th>\n",
       "      <td>1.300271</td>\n",
       "      <td>0.595224</td>\n",
       "      <td>1.291613</td>\n",
       "      <td>0.695748</td>\n",
       "      <td>-0.093505</td>\n",
       "      <td>1.121030</td>\n",
       "      <td>1.323409</td>\n",
       "      <td>0.772859</td>\n",
       "      <td>-0.772557</td>\n",
       "      <td>0.985478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104164</th>\n",
       "      <td>0.527363</td>\n",
       "      <td>0.945911</td>\n",
       "      <td>-0.348908</td>\n",
       "      <td>1.005640</td>\n",
       "      <td>-0.513735</td>\n",
       "      <td>0.081335</td>\n",
       "      <td>0.849469</td>\n",
       "      <td>0.255534</td>\n",
       "      <td>-0.516017</td>\n",
       "      <td>0.257840</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104165</th>\n",
       "      <td>-0.179149</td>\n",
       "      <td>0.534978</td>\n",
       "      <td>0.871169</td>\n",
       "      <td>1.005640</td>\n",
       "      <td>-0.790600</td>\n",
       "      <td>0.560148</td>\n",
       "      <td>1.315743</td>\n",
       "      <td>1.432477</td>\n",
       "      <td>-0.463539</td>\n",
       "      <td>-0.074979</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104166</th>\n",
       "      <td>0.405089</td>\n",
       "      <td>0.672919</td>\n",
       "      <td>0.515787</td>\n",
       "      <td>0.441163</td>\n",
       "      <td>-1.124405</td>\n",
       "      <td>1.066834</td>\n",
       "      <td>1.191548</td>\n",
       "      <td>0.433524</td>\n",
       "      <td>-0.423067</td>\n",
       "      <td>1.001890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104167</th>\n",
       "      <td>1.626382</td>\n",
       "      <td>0.472860</td>\n",
       "      <td>0.827633</td>\n",
       "      <td>0.199490</td>\n",
       "      <td>-0.410589</td>\n",
       "      <td>0.560148</td>\n",
       "      <td>1.075574</td>\n",
       "      <td>0.433859</td>\n",
       "      <td>-0.428554</td>\n",
       "      <td>0.257840</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104168</th>\n",
       "      <td>0.488112</td>\n",
       "      <td>0.268987</td>\n",
       "      <td>0.897290</td>\n",
       "      <td>1.005640</td>\n",
       "      <td>-0.448590</td>\n",
       "      <td>0.423344</td>\n",
       "      <td>0.730466</td>\n",
       "      <td>0.291199</td>\n",
       "      <td>-0.463539</td>\n",
       "      <td>0.257840</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104169</th>\n",
       "      <td>1.403967</td>\n",
       "      <td>0.153494</td>\n",
       "      <td>0.495121</td>\n",
       "      <td>0.888328</td>\n",
       "      <td>0.130793</td>\n",
       "      <td>0.787074</td>\n",
       "      <td>0.875102</td>\n",
       "      <td>0.331416</td>\n",
       "      <td>-0.324414</td>\n",
       "      <td>0.858669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104170</th>\n",
       "      <td>1.606756</td>\n",
       "      <td>0.853530</td>\n",
       "      <td>1.210744</td>\n",
       "      <td>1.005640</td>\n",
       "      <td>-0.931747</td>\n",
       "      <td>0.560148</td>\n",
       "      <td>0.768330</td>\n",
       "      <td>0.790508</td>\n",
       "      <td>-0.655959</td>\n",
       "      <td>0.457532</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104171</th>\n",
       "      <td>0.906336</td>\n",
       "      <td>0.150017</td>\n",
       "      <td>1.805179</td>\n",
       "      <td>0.651815</td>\n",
       "      <td>-0.066909</td>\n",
       "      <td>0.345278</td>\n",
       "      <td>1.316240</td>\n",
       "      <td>0.957307</td>\n",
       "      <td>-0.324077</td>\n",
       "      <td>1.004885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104172</th>\n",
       "      <td>1.005607</td>\n",
       "      <td>1.455998</td>\n",
       "      <td>0.953584</td>\n",
       "      <td>0.651815</td>\n",
       "      <td>0.014494</td>\n",
       "      <td>0.751624</td>\n",
       "      <td>1.308289</td>\n",
       "      <td>0.808599</td>\n",
       "      <td>-0.176251</td>\n",
       "      <td>0.608739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104173</th>\n",
       "      <td>1.424685</td>\n",
       "      <td>1.116107</td>\n",
       "      <td>1.965836</td>\n",
       "      <td>0.474215</td>\n",
       "      <td>-0.430391</td>\n",
       "      <td>0.154273</td>\n",
       "      <td>1.165543</td>\n",
       "      <td>1.369666</td>\n",
       "      <td>-0.699593</td>\n",
       "      <td>0.331734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104174</th>\n",
       "      <td>1.233875</td>\n",
       "      <td>0.418707</td>\n",
       "      <td>0.528328</td>\n",
       "      <td>1.005640</td>\n",
       "      <td>-0.638596</td>\n",
       "      <td>0.560148</td>\n",
       "      <td>1.577548</td>\n",
       "      <td>1.361148</td>\n",
       "      <td>-0.352753</td>\n",
       "      <td>0.723787</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104175</th>\n",
       "      <td>0.429837</td>\n",
       "      <td>0.140588</td>\n",
       "      <td>-0.085316</td>\n",
       "      <td>0.651815</td>\n",
       "      <td>-0.705602</td>\n",
       "      <td>1.216018</td>\n",
       "      <td>1.238438</td>\n",
       "      <td>1.514960</td>\n",
       "      <td>-0.579411</td>\n",
       "      <td>1.174661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104176</th>\n",
       "      <td>1.253592</td>\n",
       "      <td>1.557003</td>\n",
       "      <td>0.989985</td>\n",
       "      <td>0.888328</td>\n",
       "      <td>-0.805057</td>\n",
       "      <td>1.085619</td>\n",
       "      <td>1.072557</td>\n",
       "      <td>1.106158</td>\n",
       "      <td>0.848096</td>\n",
       "      <td>1.135375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104177</th>\n",
       "      <td>2.040844</td>\n",
       "      <td>0.807999</td>\n",
       "      <td>1.186482</td>\n",
       "      <td>0.695748</td>\n",
       "      <td>-0.495505</td>\n",
       "      <td>1.331822</td>\n",
       "      <td>1.547232</td>\n",
       "      <td>1.331634</td>\n",
       "      <td>-0.242539</td>\n",
       "      <td>1.224314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104178</th>\n",
       "      <td>0.860993</td>\n",
       "      <td>-0.028859</td>\n",
       "      <td>2.305657</td>\n",
       "      <td>1.005640</td>\n",
       "      <td>-0.204297</td>\n",
       "      <td>0.491746</td>\n",
       "      <td>1.018236</td>\n",
       "      <td>1.040163</td>\n",
       "      <td>-0.352753</td>\n",
       "      <td>0.657223</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104179</th>\n",
       "      <td>0.164631</td>\n",
       "      <td>1.538376</td>\n",
       "      <td>1.241281</td>\n",
       "      <td>0.474215</td>\n",
       "      <td>-0.571925</td>\n",
       "      <td>0.761134</td>\n",
       "      <td>1.390827</td>\n",
       "      <td>1.264645</td>\n",
       "      <td>-0.600444</td>\n",
       "      <td>0.850414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104180</th>\n",
       "      <td>1.339521</td>\n",
       "      <td>0.526050</td>\n",
       "      <td>1.185357</td>\n",
       "      <td>0.888328</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>1.346845</td>\n",
       "      <td>1.177351</td>\n",
       "      <td>1.069266</td>\n",
       "      <td>-0.051737</td>\n",
       "      <td>1.515845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104181</th>\n",
       "      <td>0.906336</td>\n",
       "      <td>1.363275</td>\n",
       "      <td>0.660412</td>\n",
       "      <td>0.651815</td>\n",
       "      <td>-0.611677</td>\n",
       "      <td>0.983821</td>\n",
       "      <td>1.381548</td>\n",
       "      <td>0.845776</td>\n",
       "      <td>-0.539095</td>\n",
       "      <td>1.231253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104182</th>\n",
       "      <td>1.164440</td>\n",
       "      <td>0.491050</td>\n",
       "      <td>1.551562</td>\n",
       "      <td>0.651815</td>\n",
       "      <td>-0.699341</td>\n",
       "      <td>1.274067</td>\n",
       "      <td>1.425276</td>\n",
       "      <td>1.440607</td>\n",
       "      <td>-0.680201</td>\n",
       "      <td>1.004885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104183</th>\n",
       "      <td>0.232983</td>\n",
       "      <td>0.716553</td>\n",
       "      <td>0.947356</td>\n",
       "      <td>1.005640</td>\n",
       "      <td>-0.454019</td>\n",
       "      <td>0.765354</td>\n",
       "      <td>1.608381</td>\n",
       "      <td>1.539472</td>\n",
       "      <td>-0.539341</td>\n",
       "      <td>0.324404</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104184</th>\n",
       "      <td>1.640269</td>\n",
       "      <td>0.749263</td>\n",
       "      <td>0.787594</td>\n",
       "      <td>0.888328</td>\n",
       "      <td>-0.484898</td>\n",
       "      <td>1.533435</td>\n",
       "      <td>1.666023</td>\n",
       "      <td>1.216836</td>\n",
       "      <td>-0.119906</td>\n",
       "      <td>1.377492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104185</th>\n",
       "      <td>0.238051</td>\n",
       "      <td>1.236988</td>\n",
       "      <td>0.719391</td>\n",
       "      <td>0.441163</td>\n",
       "      <td>-0.616718</td>\n",
       "      <td>1.484525</td>\n",
       "      <td>1.548241</td>\n",
       "      <td>1.350444</td>\n",
       "      <td>-0.402550</td>\n",
       "      <td>1.531909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104186</th>\n",
       "      <td>1.532859</td>\n",
       "      <td>1.757733</td>\n",
       "      <td>0.367602</td>\n",
       "      <td>0.888328</td>\n",
       "      <td>-0.546467</td>\n",
       "      <td>1.346845</td>\n",
       "      <td>1.328475</td>\n",
       "      <td>0.774126</td>\n",
       "      <td>0.248207</td>\n",
       "      <td>1.481257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104187</th>\n",
       "      <td>1.104877</td>\n",
       "      <td>1.610013</td>\n",
       "      <td>1.117621</td>\n",
       "      <td>0.651815</td>\n",
       "      <td>-0.066909</td>\n",
       "      <td>1.274067</td>\n",
       "      <td>1.655274</td>\n",
       "      <td>0.511184</td>\n",
       "      <td>-0.465183</td>\n",
       "      <td>1.061477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104188</th>\n",
       "      <td>0.509487</td>\n",
       "      <td>0.807840</td>\n",
       "      <td>0.140088</td>\n",
       "      <td>0.441163</td>\n",
       "      <td>0.220026</td>\n",
       "      <td>2.016132</td>\n",
       "      <td>0.844134</td>\n",
       "      <td>1.541469</td>\n",
       "      <td>-0.494878</td>\n",
       "      <td>1.779252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104189</th>\n",
       "      <td>1.482106</td>\n",
       "      <td>0.723643</td>\n",
       "      <td>0.578975</td>\n",
       "      <td>0.651815</td>\n",
       "      <td>-1.068781</td>\n",
       "      <td>1.041870</td>\n",
       "      <td>1.334412</td>\n",
       "      <td>1.440607</td>\n",
       "      <td>-0.693640</td>\n",
       "      <td>1.287846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104190 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "0       0.839484 -0.623655 -0.902766  0.470001 -0.127820 -0.380492 -0.675883   \n",
       "1      -1.660699 -0.143257 -1.192616  0.470001  0.229215 -0.625024 -0.473588   \n",
       "2      -0.229297 -0.372814 -0.577594  0.470001 -0.048896 -0.298982 -0.352687   \n",
       "3      -0.267467 -0.801524 -0.049840  0.470001 -0.063929 -0.462003 -0.908165   \n",
       "4      -0.133870 -0.334808 -0.517339  0.470001 -0.030105 -0.625024 -0.943388   \n",
       "5      -0.897285 -1.561647 -1.222743  0.470001 -0.007555 -0.706535 -0.638280   \n",
       "6      -0.897285 -0.920103 -0.213984  0.470001 -0.131578 -0.298982 -0.617812   \n",
       "7      -0.076614 -1.292564 -0.624344  0.470001 -0.244325 -0.298982 -0.547842   \n",
       "8      -1.336248  0.592542  0.558948  0.470001  0.093918 -0.217471 -0.752993   \n",
       "9      -0.973626 -0.629736 -0.858094  0.470001  0.014994 -0.380492 -0.591157   \n",
       "10     -1.030882 -1.271280 -0.318911  0.470001  0.169083 -0.380492 -0.604485   \n",
       "11     -0.076614 -0.298322 -0.943283  0.470001  0.041302 -0.543514 -0.683023   \n",
       "12     -1.221736 -0.045961 -0.307484  0.470001 -0.075204 -0.543514 -0.890078   \n",
       "13     -0.668260 -0.947468 -0.417606  0.470001  0.157808 -0.462003 -0.907689   \n",
       "14     -0.152955 -0.696627 -0.842511  0.470001 -0.033863 -0.706535 -0.560694   \n",
       "15     -0.496492 -0.742234  0.089371  0.470001 -0.157886  0.108571 -1.067621   \n",
       "16     -0.744602 -1.017399 -0.288784  0.470001  0.067610 -0.217471 -0.881510   \n",
       "17      0.018813 -0.435145 -0.333456  0.470001 -0.278150 -0.380492 -0.773461   \n",
       "18     -0.286553 -0.579568 -0.477861  0.470001 -0.139094 -0.625024 -0.831531   \n",
       "19      0.056984 -0.245114 -0.644083  0.470001  0.026269 -0.298982 -1.106652   \n",
       "20      0.228752 -0.926184 -0.799916  0.470001  0.067610 -0.298982 -0.506431   \n",
       "21     -0.248382 -1.005237 -0.295017  0.470001 -0.304458 -0.706535 -0.852951   \n",
       "22     -0.878199 -0.286160 -1.226899  0.470001  0.311897 -0.380492 -0.585921   \n",
       "23     -0.496492  0.744567 -0.701222  0.470001 -0.364590 -0.135960 -1.001935   \n",
       "24     -0.630089 -1.058446 -0.692911  0.470001 -0.180435  0.190082 -0.979087   \n",
       "25     -0.191126 -0.929225  0.121577  0.470001  0.131500 -0.054450 -0.520711   \n",
       "26     -0.954541 -0.416902 -0.684600  0.470001  0.041302  0.027061 -0.869610   \n",
       "27     -1.240821  0.738486 -0.582789  0.470001 -0.255600 -0.135960 -0.419325   \n",
       "28     -1.202651 -0.340889 -0.913155  0.470001 -0.033863 -0.054450 -0.869134   \n",
       "29      0.400520 -0.558285 -0.616033  0.470001 -0.052654  0.027061 -0.805352   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "104160  1.035780  0.948075  0.923657  0.695748 -1.104596  0.741602  0.785360   \n",
       "104161  0.390129  1.319271  1.270024  0.651815 -0.430087  0.577476  1.108957   \n",
       "104162  0.888398  1.722405  1.172488 -0.183013 -0.792743  0.787074  0.973278   \n",
       "104163  1.300271  0.595224  1.291613  0.695748 -0.093505  1.121030  1.323409   \n",
       "104164  0.527363  0.945911 -0.348908  1.005640 -0.513735  0.081335  0.849469   \n",
       "104165 -0.179149  0.534978  0.871169  1.005640 -0.790600  0.560148  1.315743   \n",
       "104166  0.405089  0.672919  0.515787  0.441163 -1.124405  1.066834  1.191548   \n",
       "104167  1.626382  0.472860  0.827633  0.199490 -0.410589  0.560148  1.075574   \n",
       "104168  0.488112  0.268987  0.897290  1.005640 -0.448590  0.423344  0.730466   \n",
       "104169  1.403967  0.153494  0.495121  0.888328  0.130793  0.787074  0.875102   \n",
       "104170  1.606756  0.853530  1.210744  1.005640 -0.931747  0.560148  0.768330   \n",
       "104171  0.906336  0.150017  1.805179  0.651815 -0.066909  0.345278  1.316240   \n",
       "104172  1.005607  1.455998  0.953584  0.651815  0.014494  0.751624  1.308289   \n",
       "104173  1.424685  1.116107  1.965836  0.474215 -0.430391  0.154273  1.165543   \n",
       "104174  1.233875  0.418707  0.528328  1.005640 -0.638596  0.560148  1.577548   \n",
       "104175  0.429837  0.140588 -0.085316  0.651815 -0.705602  1.216018  1.238438   \n",
       "104176  1.253592  1.557003  0.989985  0.888328 -0.805057  1.085619  1.072557   \n",
       "104177  2.040844  0.807999  1.186482  0.695748 -0.495505  1.331822  1.547232   \n",
       "104178  0.860993 -0.028859  2.305657  1.005640 -0.204297  0.491746  1.018236   \n",
       "104179  0.164631  1.538376  1.241281  0.474215 -0.571925  0.761134  1.390827   \n",
       "104180  1.339521  0.526050  1.185357  0.888328  0.635659  1.346845  1.177351   \n",
       "104181  0.906336  1.363275  0.660412  0.651815 -0.611677  0.983821  1.381548   \n",
       "104182  1.164440  0.491050  1.551562  0.651815 -0.699341  1.274067  1.425276   \n",
       "104183  0.232983  0.716553  0.947356  1.005640 -0.454019  0.765354  1.608381   \n",
       "104184  1.640269  0.749263  0.787594  0.888328 -0.484898  1.533435  1.666023   \n",
       "104185  0.238051  1.236988  0.719391  0.441163 -0.616718  1.484525  1.548241   \n",
       "104186  1.532859  1.757733  0.367602  0.888328 -0.546467  1.346845  1.328475   \n",
       "104187  1.104877  1.610013  1.117621  0.651815 -0.066909  1.274067  1.655274   \n",
       "104188  0.509487  0.807840  0.140088  0.441163  0.220026  2.016132  0.844134   \n",
       "104189  1.482106  0.723643  0.578975  0.651815 -1.068781  1.041870  1.334412   \n",
       "\n",
       "              7         8         9  ...    15   16   17   18   19   20   21  \\\n",
       "0      -0.981141 -0.200647 -0.460733 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "1       0.052924 -0.024352 -0.216213 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "2       0.052924 -0.100479 -0.460733 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "3      -0.705390 -0.336875 -0.297719 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "4      -0.601984 -0.028359 -0.460733 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "5      -0.774328 -0.120513 -0.053199 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "6      -0.601984 -0.084453 -0.216213 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "7      -0.946672 -0.052399 -0.216213 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "8      -0.395171 -0.076439 -0.297719 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "9      -1.187954 -0.088459 -0.216213 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "10     -0.877735 -0.108493 -0.216213 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "11     -0.739859 -0.084453 -0.216213 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "12     -0.464108  0.059789 -0.460733 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "13     -0.843266 -0.108493 -0.216213 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "14     -0.464108 -0.336875 -0.705253 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "15     -0.326233 -0.024352  0.109814 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "16     -0.739859 -0.052399 -0.542239 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "17     -0.153889 -0.032365 -0.216213 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "18     -0.808797 -0.064419 -0.623746 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "19     -0.084951  0.023728 -0.297719 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "20     -0.429640 -0.068426  0.191321 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "21     -0.774328 -0.324855 -0.379226 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "22     -1.015610 -0.088459 -0.216213 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "23     -0.670922 -0.064419  0.272827 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "24     -0.395171  0.015715 -0.053199 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "25     -0.533046 -0.108493 -0.134706 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "26     -0.498577 -0.160580 -0.542239 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "27     -1.498174 -0.152566 -0.134706 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "28     -0.222826  0.067802 -0.134706 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "29     -0.291764 -0.176607  0.109814 ...   0.0  1.0  0.0  0.0  0.0  0.0  1.0   \n",
       "...          ...       ...       ... ...   ...  ...  ...  ...  ...  ...  ...   \n",
       "104160  1.116721 -0.351261  0.627223 ...   0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "104161  0.920130 -0.216567  0.778516 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "104162  1.253729 -0.406216  0.720315 ...   0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "104163  0.772859 -0.772557  0.985478 ...   0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "104164  0.255534 -0.516017  0.257840 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "104165  1.432477 -0.463539 -0.074979 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "104166  0.433524 -0.423067  1.001890 ...   0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "104167  0.433859 -0.428554  0.257840 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "104168  0.291199 -0.463539  0.257840 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "104169  0.331416 -0.324414  0.858669 ...   0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "104170  0.790508 -0.655959  0.457532 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "104171  0.957307 -0.324077  1.004885 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "104172  0.808599 -0.176251  0.608739 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "104173  1.369666 -0.699593  0.331734 ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "104174  1.361148 -0.352753  0.723787 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "104175  1.514960 -0.579411  1.174661 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "104176  1.106158  0.848096  1.135375 ...   0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "104177  1.331634 -0.242539  1.224314 ...   0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "104178  1.040163 -0.352753  0.657223 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "104179  1.264645 -0.600444  0.850414 ...   0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "104180  1.069266 -0.051737  1.515845 ...   0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "104181  0.845776 -0.539095  1.231253 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "104182  1.440607 -0.680201  1.004885 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "104183  1.539472 -0.539341  0.324404 ...   1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "104184  1.216836 -0.119906  1.377492 ...   0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "104185  1.350444 -0.402550  1.531909 ...   0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "104186  0.774126  0.248207  1.481257 ...   0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "104187  0.511184 -0.465183  1.061477 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "104188  1.541469 -0.494878  1.779252 ...   0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "104189  1.440607 -0.693640  1.287846 ...   0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "\n",
       "         22   23   24  \n",
       "0       0.0  0.0  0.0  \n",
       "1       0.0  0.0  0.0  \n",
       "2       0.0  0.0  0.0  \n",
       "3       0.0  0.0  0.0  \n",
       "4       0.0  0.0  0.0  \n",
       "5       0.0  0.0  0.0  \n",
       "6       0.0  0.0  0.0  \n",
       "7       0.0  0.0  0.0  \n",
       "8       0.0  0.0  0.0  \n",
       "9       0.0  0.0  0.0  \n",
       "10      0.0  0.0  0.0  \n",
       "11      0.0  0.0  0.0  \n",
       "12      0.0  0.0  0.0  \n",
       "13      0.0  0.0  0.0  \n",
       "14      0.0  0.0  0.0  \n",
       "15      0.0  0.0  0.0  \n",
       "16      0.0  0.0  0.0  \n",
       "17      0.0  0.0  0.0  \n",
       "18      0.0  0.0  0.0  \n",
       "19      0.0  0.0  0.0  \n",
       "20      0.0  0.0  0.0  \n",
       "21      0.0  0.0  0.0  \n",
       "22      0.0  0.0  0.0  \n",
       "23      0.0  0.0  0.0  \n",
       "24      0.0  0.0  0.0  \n",
       "25      0.0  0.0  0.0  \n",
       "26      0.0  0.0  0.0  \n",
       "27      0.0  0.0  0.0  \n",
       "28      0.0  0.0  0.0  \n",
       "29      0.0  0.0  0.0  \n",
       "...     ...  ...  ...  \n",
       "104160  0.0  0.0  1.0  \n",
       "104161  0.0  0.0  1.0  \n",
       "104162  0.0  0.0  1.0  \n",
       "104163  0.0  0.0  1.0  \n",
       "104164  0.0  0.0  1.0  \n",
       "104165  0.0  0.0  1.0  \n",
       "104166  0.0  0.0  1.0  \n",
       "104167  0.0  0.0  1.0  \n",
       "104168  0.0  0.0  1.0  \n",
       "104169  0.0  0.0  1.0  \n",
       "104170  0.0  0.0  1.0  \n",
       "104171  0.0  0.0  1.0  \n",
       "104172  0.0  0.0  1.0  \n",
       "104173  0.0  0.0  1.0  \n",
       "104174  0.0  0.0  1.0  \n",
       "104175  0.0  0.0  1.0  \n",
       "104176  0.0  0.0  1.0  \n",
       "104177  0.0  0.0  1.0  \n",
       "104178  0.0  0.0  1.0  \n",
       "104179  0.0  0.0  1.0  \n",
       "104180  0.0  0.0  1.0  \n",
       "104181  0.0  0.0  1.0  \n",
       "104182  0.0  0.0  1.0  \n",
       "104183  0.0  0.0  1.0  \n",
       "104184  0.0  0.0  1.0  \n",
       "104185  0.0  0.0  1.0  \n",
       "104186  0.0  0.0  1.0  \n",
       "104187  0.0  0.0  1.0  \n",
       "104188  0.0  0.0  1.0  \n",
       "104189  0.0  0.0  1.0  \n",
       "\n",
       "[104190 rows x 25 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model.datareader.test_data[:, 99, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T23:14:34.375149Z",
     "start_time": "2018-04-12T23:14:33.343985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fdfa51d8f60>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLwiCUOSIohcEQShyRNELgiAUOf8fQWM0kAhV+BUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0z8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKpjjwf0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "pd.DataFrame(a[9000:10000, 99, 0]).plot()\n",
    "pd.DataFrame(b[9000:10000, 99, 0]).plot()\n",
    "\n",
    "pd.DataFrame((b-a)[:, 99, 0]).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T23:15:26.219412Z",
     "start_time": "2018-04-12T23:15:26.146739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-142.34136963],\n",
       "        [   3.1204834 ],\n",
       "        [   6.64770508],\n",
       "        ...,\n",
       "        [  -0.33026123],\n",
       "        [  -0.33172607],\n",
       "        [  27.80221558]],\n",
       "\n",
       "       [[-142.34136963],\n",
       "        [   3.1204834 ],\n",
       "        [   6.64770508],\n",
       "        ...,\n",
       "        [  -0.33026123],\n",
       "        [  27.80244446],\n",
       "        [ -51.11155701]],\n",
       "\n",
       "       [[-142.34136963],\n",
       "        [   3.1204834 ],\n",
       "        [   6.64770508],\n",
       "        ...,\n",
       "        [  27.80265808],\n",
       "        [ -51.11152649],\n",
       "        [ -61.67979431]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ -38.76849365],\n",
       "        [ -65.93013   ],\n",
       "        [ -20.49487305],\n",
       "        ...,\n",
       "        [   2.38899994],\n",
       "        [   1.35089874],\n",
       "        [   0.36315155]],\n",
       "\n",
       "       [[  -8.12461853],\n",
       "        [ -29.9694519 ],\n",
       "        [ -27.59677124],\n",
       "        ...,\n",
       "        [ -11.51211548],\n",
       "        [ -12.42169189],\n",
       "        [ -13.40569305]],\n",
       "\n",
       "       [[ -55.86302185],\n",
       "        [ -21.84295654],\n",
       "        [ -44.43746948],\n",
       "        ...,\n",
       "        [ -12.41233444],\n",
       "        [ -13.39479446],\n",
       "        [ -14.49444962]]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
